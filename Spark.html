<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><title>Welcome to</title><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" name="viewport"><link href="reveal.js/css/reveal.css" rel="stylesheet"><link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme"><!--This CSS is generated by the Asciidoctor-Reveal.js converter to further integrate AsciiDoc's existing semantic with Reveal.js--><style type="text/css">.reveal div.right {
  float: right;
}

/* callouts */
.conum[data-value] {display:inline-block;color:#fff!important;background-color:rgba(50,150,50,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}</style><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css"><link href="reveal.js/lib/css/zenburn.css" rel="stylesheet"><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? "reveal.js/css/print/pdf.css" : "reveal.js/css/print/paper.css";
document.getElementsByTagName( 'head' )[0].appendChild( link );</script><!--[if lt IE 9]><script src="reveal.js/lib/js/html5shiv.js"></script><![endif]--><link rel="stylesheet" href="./styles/formatting.css"></head><body><div class="reveal"><div class="slides"><section class="title" data-state="title"><h1>Welcome to</h1><div class="preamble"><div class="paragraph big"><p>Understanding</p></div>
<div class="imageblock" style=""><img src="./images/title/apache_spark_logo_background.png" alt="Spark Logo"></div>
<div class="paragraph big"><p>Internals</p></div></div></section>
<section><section id="_before_starting"><h2>Before starting &#8230;&#8203;</h2></section><section id="_me"><h2>Me</h2><div class="paragraph"><p>Arnaud Larroque<br></p></div>
<div class="paragraph"><p>Big Data / Backend developer<br></p></div>
<div class="paragraph"><p>Freelance<br></p></div>
<div class="paragraph"><p>Currently working with <strong>Adneom</strong> as contractor for <strong>Carrefour Big data team</strong></p></div>
<div class="paragraph"><p>Email me : <a href="mailto:alarroque@gmail.com">alarroque@gmail.com</a><br>
Follow me: @nonontb</p></div></section><section id="_this_talk"><h2>This talk</h2><div class="paragraph"><p>More about <strong>Apache Spark internals</strong> than learning how to use it !<br></p></div>
<div class="paragraph"><p>Handcrafted presentation by <strong>me &amp; my daugther</strong></p></div></section><section id="_lets_dive"><h2>Let&#8217;s dive &#8230;&#8203;</h2></section></section>
<section><section id="_a_cluster_computing_system"><h2>A cluster computing system</h2><div class="paragraph"><p>Spark is a <strong>framework</strong> which will help coordinate <strong>cluster</strong> to act as a <strong>single one</strong></p></div><div class="paragraph"><p>Why a Cluster ?</p></div><div class="paragraph"><p>There are tasks as data processing that the best computer can&#8217;t handle</p></div></section><section id="_spark_cluster_manager"><h2>Spark cluster manager</h2><div class="paragraph"><p>All the cumulative resources of a cluster doesn&#8217;t do anything alone.
You need something to manage all available nodes composing a cluster<br></p></div>
<div class="ulist"><ul><li><p>Spark standalone</p></li><li><p>Hadoop Yarn</p></li><li><p>Apache Mesos</p></li><li><p>Kubernetes (experimental) : container based manager / orchestrator</p></li></ul></div>
<aside class="notes"><div class="paragraph"><p>There is also a local deployment &#8658; mainly testing purpose as Driver/Executors
are replaced by thread</p></div></aside></section><section id="_spark_application"><h2>Spark application</h2><div class="paragraph"><p>Is a <strong>driver</strong> and some <strong>executors</strong></p></div>
<div class="paragraph"><p>Many Spark applications can be run in the same cluster concurrently</p></div></section><section id="_driver_role"><h2>Driver role</h2><div class="ulist"><ul><li><p>Runs user code (<code><code>main()</code></code>) and build a <strong>DAG</strong></p></li><li><p>Maintains cluster state</p></li><li><p>Schedules and distributes all tasks</p></li></ul></div>
<aside class="notes"><div class="ulist"><ul><li><p>Do not talk too much about DAG</p></li></ul></div></aside></section><section id="_executor_role"><h2>Executor role</h2><div class="ulist"><ul><li><p>Read / Write / Store data</p></li><li><p>Execute what the driver assigns them</p></li><li><p>Send metrics back to the driver</p></li></ul></div></section><section data-transition="fade-in none-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/cluster_overview/1.svg" alt="Cluster Overview 1" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/cluster_overview/2.svg" alt="Cluster Overview 2" width="background" height="600px"></div></section><section data-transition="none-in fade-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/cluster_overview/3.svg" alt="Cluster Overview 5" width="background" height="600px"></div></section></section>
<section><section id="_spark_core_component_rdd"><h2>Spark core component : RDD</h2><div class="paragraph big"><p>RDD as <strong><em>Resilient distributed dataset</em></strong></p></div><div class="paragraph"><p>Core Spark data structure</p></div><div class="paragraph"><p>No matter which API is used,<br>
It will end producing RDDs</p></div></section><section id="_rdd_what_is_it_exactly"><h2>RDD: What is it exactly ?</h2><div class="paragraph"><p>Made up of 4 attributes :</p></div>
<div class="ulist"><ul><li class="fragment"><p>Dependencies : Relation between a RDD and the one it is derived from</p></li><li class="fragment"><p>Partitions : subset of dataset located on cluster node</p></li><li class="fragment"><p>Function : transform this RDD from its parents version</p></li><li class="fragment"><p>Metadata : Partitioning scheme and data locality</p></li></ul></div></section><section id="_dependencies"><h2>Dependencies</h2><div class="paragraph"><p>RDDs <strong>Dependencies</strong> stand for its <strong>Resilience</strong></p></div>
<aside class="notes"><div class="paragraph"><p>It keeps track of all previous steps
We can rewind all dependencies and restart from the beginning</p></div></aside></section><section id="_partitions"><h2>Partitions</h2><div class="ulist"><ul><li><p>A dataset <strong>chunk</strong> located on a cluster node</p></li><li><p>Define along the cluster size the <strong>parallelism</strong></p></li></ul></div>
<aside class="notes"><div class="ulist"><ul><li><p>Partitioner defines in which partition data will end</p></li><li><p>default partitioner: HashCode modulo nb_partitions</p></li><li><p>Relation between File and partition : Splittable, block, Partitioner, &#8230;&#8203;</p></li></ul></div></aside></section><section data-transition="fade-in none-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/rdd/1.svg" alt="RDD 1" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/rdd/2.svg" alt="RDD 2" width="background" height="600px"></div></section><section data-transition="none-in fade-out"><div class="imageblock" style=""><img src="./images/rdd/3.svg" alt="RDD 3" width="background" height="600px"></div></section><section data-transition="none-in fade-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/rdd/4.svg" alt="RDD 4" width="background" height="600px"></div></section><section id="_typed_collection"><h2>Typed collection</h2><div class="paragraph"><p><strong>Typed safe</strong> many errors arise at <strong>compile-time</strong></p></div>
<div class="paragraph"><p><strong>Designed to be used as a Scala collection</strong></p></div>
<div class="paragraph"><p>Can handle Structured data and unique choice unstructured data (ie Binary)</p></div>
<aside class="notes"><div class="paragraph"><p>Most known scala collection API methods <code>map</code>, <code>flatMap</code>, <code>filter</code>, &#8230;&#8203; apply to RDD
meaning use as a non distributed abstraction</p></div></aside></section><section id="_when_to_use"><h2>When to use</h2><div class="paragraph"><p>When you need <strong>control &amp; flexibility</strong> over built-in <strong>performance</strong>,
<strong>resources optimization</strong> and <strong>code expressiveness</strong></p></div>
<div class="paragraph"><p>Best for unstructured data == Binary</p></div>
<aside class="notes"><div class="paragraph"><p>See later with tungsten and Catalyst optimizer
Unstructured data &#8658; Binary</p></div></aside></section></section>
<section><section id="_running_code_in_spark"><h2>Running code in Spark</h2><div class="paragraph"><p><strong>Code you write is not directly run !</strong></p></div><div class="paragraph"><p>Spark will used it to build <strong>DAG == Directed Acyclic Graph</strong></p></div><div class="imageblock" style=""><img src="./images/giphy.webp" alt="Whaaaat?"></div><aside class="notes"><div class="paragraph"><p>Try to explain it with debugger use case.</p></div></aside></section><section id="_a_dag"><h2>A DAG ?</h2><div class="paragraph"><p><strong>RDD lineage</strong> or RDDs dependencies Graph</p></div>
<div class="ulist"><ul><li><p>Vertice = Immutable '<em>version</em>' of a RDD</p></li><li><p>Edge = Transformations between RDDs</p></li><li><p>No way back</p></li><li><p>Execution triggered by an <strong>Action</strong></p></li></ul></div></section><section id="_spark_operation_transformation"><h2>Spark Operation : transformation</h2><div class="ulist"><ul><li><p>Lazy evaluated</p></li><li><p>Return a new <strong>immutable RDD</strong></p></li><li><p>Can be a <strong>Narrow</strong> or <strong>Wide</strong> dependencies</p></li></ul></div>
<aside class="notes"><div class="literalblock"><div class="content"><pre>nothing is done when called, just another part of the Dag is built</pre></div></div></aside></section><section id="_narrow"><h2>Narrow</h2><div class="paragraph"><p>Each partition of the parent RDD <strong>is used by at most one partition</strong> of the child RDD</p></div></section><section id="_wide"><h2>Wide</h2><div class="paragraph"><p>Each partition of the parent RDD <strong>may be used by multiple child</strong> partitions</p></div></section><section id="_narrow_vs_wide_transformation"><h2>Narrow vs Wide transformation</h2><div class="paragraph"><p>Narrow transformation can be <strong>pipelined</strong> while wide transformation requires <strong>shuffling data</strong></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>pipelined transformations into one stage</p></li><li><p>shuffling delimits stages</p></li><li><p>Wide dependencies goes with performance impact as date is redistributed across cluster nodes</p></li></ul></div></aside></section><section><div class="imageblock" style=""><img src="./images/dependencies/Narrow.svg" alt="narrow" width="background" height="600px"></div></section><section><div class="imageblock" style=""><img src="./images/dependencies/Wide.svg" alt="wide" width="background" height="600px"></div></section><section id="_spark_operation_actions"><h2>Spark Operation : Actions</h2><div class="ulist"><ul><li><p>Action values are stored to driver or external storage system</p></li><li><p><strong>Trigger DAG execution</strong> and do not produce new RDD</p></li></ul></div>
<aside class="notes"><div class="ulist"><ul><li><p>Do not store huge dataset on driver. Driver require small memory footprint</p></li><li><p>Collect may be a driver killer, Increase driver memory is mostly a
misconception</p></li></ul></div></aside></section><section id="_dag_hello_world"><h2>DAG: Hello world!</h2><div class="imageblock" style=""><img src="./images/dag/dag.png" alt="Directed acyclic graph" width="background" height="550px"></div>
<aside class="notes"><div class="paragraph"><p>Ask which famous job this DAG do ?</p></div></aside></section><section id="_wordcount_what_else"><h2>Wordcount, What else ?!</h2><pre class="highlight listingblock"><code data-noescape class="scala language-scala">sc.textFile("hdfs://...")
 .flatMap(line =&gt; line.split(" "))
 .map(word =&gt; (word, 1)) // read, map and flatmap in one stage
 .reduceByKey(_ + _) // shuffle =&gt; new stage
 .saveAsTextFile("hdfs://...") // Nothing is done before</code></pre></section><section id="_hello_world_dag_on_a_cluster"><h2>Hello world! DAG on a Cluster</h2></section><section data-transition="fade-in none-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/dag_run/1.svg" alt="Dag run 1" width="background" height="600px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>A stage is serialized into tasks, one for each executor</p></li><li><p>textFile, flatMap, map are pipelined</p></li></ul></div></aside></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/dag_run/2.svg" alt="Dag run 2" width="background" height="600px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>A shuffle occurs before reduceByKey</p></li></ul></div></aside></section><section data-transition="none-in fade-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/dag_run/3.svg" alt="Dag run 3" width="background" height="600px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>Reduce is perform then saveAsTextFile on Hdfs</p></li></ul></div></aside></section></section>
<section><section id="_need_for_high_level_api"><h2>Need for High-level API</h2><div class="ulist"><ul><li class="fragment"><p>RDD is about <strong>How</strong> do things but not <strong>What</strong> to do</p></li><li class="fragment"><p>Non-jvm language (Python, R, more to come &#8230;&#8203;) run slower</p></li><li class="fragment"><p>Are you certain <strong>How you do things</strong> is the best way ? <em>(I mean &#8230;&#8203;)</em></p></li></ul></div><aside class="notes"><div class="ulist"><ul><li><p>RDD: low-level API with no leverage for optimization</p></li></ul></div></aside></section><section id="_dataframe_dataset_api"><h2>Dataframe / Dataset API</h2><div class="paragraph"><p>Dataframe is still an <strong>immutable distributed data collection</strong>
but data is abstracted as <strong>table in relational database</strong></p></div></section><section id="_dataframe_dataset_features"><h2>Dataframe / Dataset features</h2><div class="ulist"><ul><li><p>Structured data with an adhoc schema</p></li><li><p>Provide a DSL API for code</p></li><li><p>Similar to well-known tools from other languages (Pandas, dplyr)</p></li><li><p>Access audience other than Data engineer</p></li><li><p>Leverage <strong>Catalyst optimization</strong> and <strong>tungsten</strong> improvements</p></li></ul></div></section><section id="_unified_api"><h2>Unified API</h2><div class="paragraph"><p>Unified DataFrame and Dataset API since Spark 2.0</p></div>
<pre class="highlight listingblock"><code data-noescape class="scala language-scala">Dataframe == Dataset[Row]</code></pre>
<div class="paragraph"><p>RDD Type-safe / functionnal programming and Dataframe DSL code optimization</p></div></section></section>
<section><section id="_catalyst_optimizer"><h2>Catalyst optimizer</h2><div class="ulist"><ul><li class="fragment"><p>Trees abstraction library which represents a user program</p></li><li class="fragment"><p>Execution engine which powers Dataframe / Dataset API and Spark SQL</p></li><li class="fragment"><p>Composed of 4 phases :</p><div class="ulist"><ul><li><p>3 phases based of Trees transformation by applying Rules</p></li><li><p>1 final step to generate byte code</p></li></ul></div></li><li class="fragment"><p>Designed to be easily extended</p></li></ul></div><aside class="notes"><div class="paragraph"><p>Dataframe API for all language &#8658; Scala, Python, R, &#8230;&#8203;</p></div></aside></section><section id="_overview"><h2>Overview</h2><div class="imageblock" style=""><img src="./images/catalyst/overview/Catalyst_overview.svg" alt="Catalyst_overview" width="background" height="550px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>Designed to be extensible and open to other languages(ie Frontend) and optimized
according to hardware (backend)</p></li></ul></div></aside></section><section id="_analysis"><h2>Analysis</h2><div class="paragraph"><p>Any <em>frontend API</em> get translated to<br>
<strong>Unresolved logical plan</strong> aka an <strong>Unresolved query plan</strong></p></div>
<div class="paragraph"><p>Meaning:</p></div>
<div class="ulist"><ul><li><p>Check the Catalog for columns names and Tables</p></li><li><p>Resolves (inner) IDs to same values</p></li><li><p>&#8230;&#8203;</p></li></ul></div>
<aside class="notes"><div class="ulist"><ul><li><p>Spark SQL &#8658; SQL Parser &#8658; Tree</p></li><li><p>DataFrame (Any Language) &#8658; Relation</p></li><li><p>unresolved logical plan &#8658; Tree</p></li></ul></div></aside></section><section id="_logical_optimization"><h2>Logical Optimization</h2><div class="paragraph"><p>Optimized logical plan is generated using standard-rules as:</p></div>
<div class="ulist"><ul><li><p>Constant folding</p></li><li><p>Predicate pushdown</p></li><li><p>Projection pruning</p></li><li><p>Null propagation</p></li><li><p>Boolean expression simplification</p></li><li><p>&#8230;&#8203;</p></li></ul></div></section><section id="_engine_based_on_rules"><h2>Engine based on Rules</h2><div class="paragraph"><p>Let&#8217;s keep it simple !</p></div>
<pre class="highlight listingblock"><code data-noescape class="scala language-scala">T0 =&gt; R1 =&gt; T1 =&gt; R2 =&gt; T2 =&gt; ... =&gt; Rn =&gt; Tn</code></pre>
<div class="paragraph"><p>We take a Tree of type T and transform it using a rule to obtain a new Tree<br>
type of T1</p></div>
<div class="paragraph"><p>And so on until Tn-1 == Tn &#8658; call a <strong>fixed point</strong></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>Rules can be composable and run in Batch</p></li><li><p>Based on scala functionnal programming &#8658; pattern matching</p></li></ul></div></aside></section><section id="_and_tree"><h2>&#8230;&#8203;and Tree</h2><div class="imageblock" style=""><img src="./images/catalyst/catalyst_tree.png" alt="Catalyst tree"></div>
<pre class="highlight listingblock"><code data-noescape class="java language-java">tree.transform {
   case Add(Literal(c1), Literal(c2)) =&gt; Literal(c1+c2)
}</code></pre></section><section data-transition="fade-in none-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/catalyst/engine/1.svg" alt="Catalyst engine 1" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/catalyst/engine/2.svg" alt="Catalyst engine 2" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/catalyst/engine/3.svg" alt="Catalyst engine 3" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/catalyst/engine/4.svg" alt="Catalyst engine 4" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/catalyst/engine/5.svg" alt="Catalyst engine 5" width="background" height="600px"></div></section><section data-transition="none-in fade-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/catalyst/engine/6.svg" alt="Catalyst engine 6" width="background" height="600px"></div></section><section id="_physical_planning"><h2>Physical Planning</h2><div class="paragraph"><p><strong>Physical plan &#8658; DAG</strong> as we saw earlier</p></div>
<div class="paragraph"><p>Use a cost-based selection principle by generating many plans and
using cost model</p></div></section><section id="_code_generation"><h2>Code Generation</h2><div class="paragraph"><p>Thanks to <strong>Tungsten project</strong> to produce optimized bytecode</p></div>
<div class="paragraph"><p>Tackle Performance overhead of Generic evaluation: Object creation and virtual function calls
and Memory consumption: primitive boxing</p></div>
<aside class="notes"><div class="ulist"><ul><li><p>See tungsten optimization later</p></li></ul></div></aside></section></section>
<section><section id="_spark_memory_management"><h2>Spark memory management</h2></section><section id="_overview_2"><h2>Overview</h2><div class="imageblock" style=""><img src="./images/spark_memory/Spark_memory_detail.svg" alt="Overview" width="background" height="550px"></div>
<aside class="notes"><div class="paragraph"><p>Explain all memory regions</p></div></aside></section><section id="_memory_in_numbers"><h2>Memory in numbers</h2><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Requested(Yarn)</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">10 + max(0.1%* x 10g,384M) = <strong>11g</strong></p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Executor memory</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">10G</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Overhead</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">300Mb</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Execution / Storage</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">(10 - 0.3) * 0.6 = <strong>5,8G</strong></p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Reserved</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">(10 - 0.3) - 5.8g = 3.9G</p></td></tr></table>
<aside class="notes"><div class="ulist"><ul><li><p>spark.memory.fraction = 0.6 &amp; spark.storage.memoryFraction = 0.5</p></li><li><p>spark.executor.memoryOverhead	executorMemory * 0.10, with minimum of 384</p></li><li><p>yarn.scheduler.minimum-allocation-mb = 1024</p></li></ul></div></aside></section><section id="_execution_vs_storage"><h2>Execution vs Storage</h2><div class="ulist"><ul><li><p>Execution memory : used for sorting, shuffling, joining, aggregating</p></li><li><p>Storage memory : Caching, Broadcasting variable</p></li></ul></div></section><section id="_unified_memory_model"><h2>Unified memory model</h2><div class="paragraph"><p>Storage and Execution can use all the <strong>available</strong> heap memory</p></div>
<div class="paragraph"><p>but, what happens on memory starvation ?</p></div></section><section data-transition="fade-in none-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/spark_memory/1.svg" alt="Unified memory model 1" width="background" height="550px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/spark_memory/2.svg" alt="Unified memory model 2" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/spark_memory/3.svg" alt="Unified memory model 3" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/spark_memory/4.svg" alt="Unified memory model 4" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/spark_memory/5.svg" alt="Unified memory model 5" width="background" height="600px"></div></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/spark_memory/6.svg" alt="Unified memory model 6" width="background" height="600px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>Caching is not always used, Execution result will always be needed, so Storage is evicted</p></li><li><p>User often over caches stuff and don&#8217;t use it</p></li></ul></div></aside></section><section data-transition="none-in none-out"><div class="imageblock" style=""><img src="./images/spark_memory/7.svg" alt="Unified memory model 7" width="background" height="600px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>You can choose what fraction of memory is immune to evict, but you&#8217;ll face more Spilling and OOM</p></li></ul></div></aside></section><section data-transition="none-in fade-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/spark_memory/8.svg" alt="Unified memory model 8" width="background" height="600px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>If all Reserved memory is not used, it is still useable</p></li></ul></div></aside></section><section id="_caching_and_checkpointing"><h2>Caching and Checkpointing</h2><div class="paragraph"><p>two useful optimizations techniques</p></div></section><section id="_caching"><h2>Caching</h2><div class="paragraph"><p>As Spark is memory-based, caching is a common optimization techniques</p></div>
<div class="ulist"><ul><li><p>Use storage memory</p></li><li><p>Prevent slow I/O shuffling and disk</p></li><li><p>Preserve RDD lineage</p></li></ul></div></section><section data-transition="fade-in none-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/dag/1.svg" alt="dag1" width="background" height="600px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>Just a random Dag, but one step has been cache</p></li><li><p>Good optimization to cache, a co-partitioned Dataset as it is an expensive task</p></li></ul></div></aside></section><section data-transition="none-in fade-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/dag/2.svg" alt="Unified memory model 7" width="background" height="600px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>See what happens if a cached partition is lost</p></li><li><p>Missing partitions will be recomputed from its parent</p></li></ul></div></aside></section><section id="_checkpointing"><h2>Checkpointing</h2><div class="paragraph"><p>Checkpointing save a <strong>point in RDD history</strong></p></div>
<div class="ulist"><ul><li><p>Store to disk, so more memory available but slow to save</p></li><li><p>Survive after driver restart</p></li></ul></div></section><section data-transition="none-in fade-out" data-transition-speed="fast"><div class="imageblock" style=""><img src="./images/dag/3.svg" alt="Unified memory model 8" width="background" height="600px"></div>
<aside class="notes"><div class="ulist"><ul><li><p>Checkpointing can also be useful to not recompute expensive task</p></li><li><p>It does not require memory, but I/O Disk</p></li><li><p>RDD lineage is lost</p></li></ul></div></aside></section></section>
<section><section id="_project_tungsten"><h2>Project Tungsten</h2><div class="paragraph"><p><strong>Tungsten</strong> was release in <strong>Apache Spark 1.5/6</strong> in 2015/16
This is not new Stuff</p></div></section><section id="_hardware_evolution"><h2>Hardware evolution</h2><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:25%"><col style="width:25%"><col style="width:25%"><col style="width:25%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"></td><td class="tableblock halign-left valign-top"><p class="tableblock">2010</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">2015</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">~2020</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Disk</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">50MB/s (HDD)</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">500 MB/S (SSD)</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">500 MB/S (SSD)</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">Network</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">1GB/s</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">10 GB/s</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">10 GB/s</p></td></tr><tr><td class="tableblock halign-left valign-top"><p class="tableblock">CPU</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">~3Ghz</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">~3GHz more cores</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">~4Ghz even more cores</p></td></tr></table></section><section id="_new_bottlenecks_cpu"><h2>New Bottlenecks: CPU</h2><div class="ulist"><ul><li><p>Networks and disks are 10x faster</p></li><li><p>Spark I/Os were optimized</p><div class="ulist"><ul><li><p>Optimized reading of input data: Pruning partition</p></li><li><p>Optimized shuffle implementation + network layer (v 1.2)</p></li></ul></div></li><li><p>Data formats like Parquet / Orc improvements</p></li><li><p>Serializing, Hashing are CPU bound workloads</p></li></ul></div></section><section id="_tungsten_focus"><h2>Tungsten focus</h2><div class="ulist"><ul><li><p>Custom memory management and Binary Processing</p></li><li><p>Cache-aware computation</p></li><li><p>Code generation</p></li></ul></div></section><section id="_jvm_caveats"><h2>JVM Caveats</h2><div class="paragraph"><p>Spark run on the JVM and favors in-memory computation</p></div>
<div class="paragraph"><p>It relies on JVM Heap which comes with <strong>JVM Object overhead</strong> and <strong>Garbage collection</strong></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>a 4-byte string used 48 bytes in memory with JVM overhead</p></li><li><p>Famous stop of the world Full GC</p></li></ul></div></aside></section><section id="_tungsten_solutions"><h2>Tungsten solutions</h2><div class="ulist"><ul><li class="fragment"><p>Use a custom binary format to store object in memory</p></li><li class="fragment"><p>Use custom C-Style manual memory management <em>sun.misc.Unsafe</em> API &amp; off-heap memory (Optional)</p></li><li class="fragment"><p>Translate Spark operations to act directly on their binary format</p></li></ul></div>
<aside class="notes"><div class="ulist"><ul><li><p>Binary format, better memory footprint</p></li><li><p>Decide exactly where data is stored and how</p></li><li><p>Encoders from Dataset API are bridges from JVM Object model to binary memory
format and quite optimized</p></li></ul></div></aside></section><section id="_cache_aware_computation"><h2>Cache-aware computation</h2><div class="paragraph"><p>CPU Caches are orders of magnitude faster than main memory access</p></div>
<div class="paragraph"><p>Cache-aware computation means create <strong>algorithms to favor sequential memory access</strong>
and <strong>optimize use of CPU L1/L2/L3 caches</strong> to process Spark custom binary format</p></div>
<aside class="notes"><div class="ulist"><ul><li><p>These algorithms are directly linked to the custom binary data structure</p></li><li><p>increase efficiency of all use cases.</p></li></ul></div></aside></section><section id="_code_generation_2"><h2>Code Generation</h2><div class="paragraph"><p><strong>Tackle Generic code cost using metadata to produce specific optimized bytecode</strong></p></div>
<div class="ulist"><ul><li><p>Improve expression evaluation as seen in the catalyst optimizer</p></li><li><p>Improve performance conversion between in-memory format to wire-protocol (Serialization)</p></li><li><p>Add compiler backends as LLVM to leverages instructions on modern CPUs/GPUs</p></li></ul></div>
<aside class="notes"><div class="ulist"><ul><li><p>Personal definition</p></li><li><p>Metadata &#8658; semantics, schema</p></li></ul></div></aside></section></section>
<section><section id="_next"><h2>Next&#8230;&#8203;</h2></section><section id="_incoming_v3_0"><h2>Incoming V3.0</h2><div class="ulist"><ul><li><p>DatasourceV2 API: Unified Streaming/Batch datasource</p></li><li><p>Spark on Kubernetes</p></li><li><p>Project Hydrogen : Unified AI/Big Data</p></li><li><p>Support for hadoop 3.X</p></li><li><p>Adaptive execution</p></li></ul></div></section><section id="_innovative_ecosysteme"><h2>Innovative ecosysteme</h2><div class="ulist"><ul><li><p>Delta Lake storage: reliable data and Acid transactions</p></li><li><p>Koalas: Scalable pandas on Spark</p></li></ul></div></section></section>
<section><div class="paragraph"><p><strong>Thank you</strong></p></div></section>
<section id="_questions"><h2>Questions</h2><div class="imageblock" style=""><img src="./images/questions.svg" alt="Questions" width="background" height="550px"></div></section></div></div><script src="reveal.js/lib/js/head.min.js"></script><script src="reveal.js/js/reveal.js"></script><script>Array.prototype.slice.call(document.querySelectorAll('.slides section')).forEach(function(slide) {
  if (slide.getAttribute('data-background-color')) return;
  // user needs to explicitly say he wants CSS color to override otherwise we might break custom css or theme (#226)
  if (!(slide.classList.contains('canvas') || slide.classList.contains('background'))) return;
  var bgColor = getComputedStyle(slide).backgroundColor;
  if (bgColor !== 'rgba(0, 0, 0, 0)' && bgColor !== 'transparent') {
    slide.setAttribute('data-background-color', bgColor);
    slide.style.backgroundColor = 'transparent';
  }
})

// See https://github.com/hakimel/reveal.js#configuration for a full list of configuration options
Reveal.initialize({
  // Display presentation control arrows
  controls: true,
  // Help the user learn the controls by providing hints, for example by
  // bouncing the down arrow when they first encounter a vertical slide
  controlsTutorial: true,
  // Determines where controls appear, "edges" or "bottom-right"
  controlsLayout: 'bottom-right',
  // Visibility rule for backwards navigation arrows; "faded", "hidden"
  // or "visible"
  controlsBackArrows: 'faded',
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: false,
  // Control which views the slide number displays on
  showSlideNumber: 'all',
  // Push each slide change to the browser history
  history: false,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // Randomizes the order of slides each time the presentation loads
  shuffle: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags whether to include the current fragment in the URL,
  // so that reloading brings you to the same fragment position
  fragmentInURL: false,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Flags if we should show a help overlay when the questionmark
  // key is pressed
  help: true,
  // Flags if speaker notes should be visible to all viewers
  showNotes: false,
  // Global override for autolaying embedded media (video/audio/iframe)
  // - null: Media will only autoplay if data-autoplay is present
  // - true: All media will autoplay, regardless of individual setting
  // - false: No media will autoplay, regardless of individual setting
  autoPlayMedia: null,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Use this method for navigation when auto-sliding
  autoSlideMethod: Reveal.navigateNext,
  // Specify the average time in seconds that you think you will spend
  // presenting each slide. This is used to show a pacing timer in the
  // speaker view
  defaultTiming: 120,
  // Enable slide navigation via mouse wheel
  mouseWheel: false,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  // Add `data-preview-link` and `data-preview-link="false"` to customise each link
  // individually
  previewLinks: false,
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',
  // Number of pixels to move the parallax background per slide
  // - Calculated automatically unless specified
  // - Set to 0 to disable movement along an axis
  parallaxBackgroundHorizontal: null,
  parallaxBackgroundVertical: null,
  // The display mode that will be used to show slides
  display: 'block',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 960,
  height: 700,

  // Factor of the display size that should remain empty around the content
  margin: 0.1,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.5,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: 'reveal.js/plugin/notes/notes.js', async: true },
      
      
      
      
  ],

  

});</script></body></html>